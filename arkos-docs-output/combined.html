<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>combined</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<h1 id="noa-executivecommanderchiefagent">NOA —
ExecutiveCommanderChiefAgent</h1>
<h2 id="definition-purpose">Definition &amp; Purpose</h2>
<p>NOA (sometimes called the
<strong>ExecutiveCommanderChiefAgent</strong>) is the top‑level
orchestrator of the <strong>ark‑os‑noa</strong> platform. It acts like a
CEO for the agent ecosystem: it translates high‑level business goals
into concrete plans, delegates work to Board Agents and
<strong>MicroAgentStacks</strong>, and ensures that every deliverable
meets business, technical, and compliance requirements.</p>
<h2 id="framework">Framework</h2>
<ul>
<li><strong>Inputs:</strong> high‑level goals, success criteria,
budgets, SLAs, risk appetite and constraints. NOA normalises these into
a <strong>WorkPlan</strong>. Each plan captures tasks, checkpoints,
deadlines and deliverables.</li>
<li><strong>Outputs:</strong> action plans, stack assignments,
acceptance tests and post‑mortems. For each goal NOA produces a package
of artefacts (e.g. zip file and compiled PDF).</li>
<li><strong>Control loop:</strong> Sense → Plan → Act → Verify → Report.
NOA constantly senses progress and risks, replans when necessary, acts
by spawning or destroying <strong>MicroAgentStacks</strong>, verifies
outputs against acceptance criteria, and finally reports to the business
owner.</li>
</ul>
<h2 id="goals">Goals</h2>
<ol type="1">
<li><strong>Disambiguate and decompose:</strong> convert ambiguous goals
into measurable objectives and step‑by‑step tasks.</li>
<li><strong>Resource allocation:</strong> assign Board Agents and
MicroAgentStacks based on domain expertise, constraints and
availability.</li>
<li><strong>Policy enforcement:</strong> apply safety, security and
legal policies; ensure no Docker‑in‑Docker
(<strong>Capsule/Full‑Illusion</strong> pattern) and maintain audit
logs.</li>
<li><strong>Model selection:</strong> orchestrate
<strong>ModelSelectorAgents</strong> to pick appropriate AI models for
each task, balancing accuracy, latency and cost.</li>
<li><strong>Packaging &amp; archiving:</strong> guarantee that outputs
are packaged into deliverable artefacts (zip + PDF) and stored
internally.</li>
</ol>
<h2 id="capabilities">Capabilities</h2>
<ul>
<li><strong>Decomposition &amp; scheduling:</strong> build dependency
graphs, schedule tasks across stacks and board seats, and respect
deadlines.</li>
<li><strong>Auto‑retry &amp; escalation:</strong> detect failures or
blockers and retry tasks with backoff; when automation fails, summarise
context and ask for human input.</li>
<li><strong>Observability:</strong> generate unique run IDs, attach
traces and metrics, and centralise logs for all stacks.</li>
<li><strong>Safety &amp; compliance:</strong> enforce licensing,
vulnerability thresholds and secret scanning. Use outer BuildKit and
containerd with sidecars rather than nested containers to avoid security
risks【43537238352704†L1068-L1088】.</li>
</ul>
<h2 id="objects-definitions">Objects &amp; Definitions</h2>
<ul>
<li><strong>WorkPlan:</strong> a structured representation of a goal →
tasks → checkpoints → deliverables → review gates.</li>
<li><strong>Assignment:</strong> mapping between Board Agents,
MicroAgentStacks and tasks; includes SLAs and ownership.</li>
<li><strong>Trace:</strong> evidence of inputs, actions, tools, models
and outputs for audit and reproducibility.</li>
</ul>
<h2 id="lifecycle">Lifecycle</h2>
<ol type="1">
<li><strong>Intake &amp; Normalise:</strong> accept a business goal and
convert it into a WorkPlan.</li>
<li><strong>Resource Match:</strong> choose which Board Agents and
stacks are needed and spin them up.</li>
<li><strong>Execution:</strong> coordinate tasks across microservices;
check progress with periodic checkpoints.</li>
<li><strong>Validation &amp; Packaging:</strong> verify results, run
security and licence scans, and package deliverables.</li>
<li><strong>Report &amp; Archive:</strong> summarise results, produce a
post‑run report, archive artefacts with retention policies.</li>
</ol>
<h2 id="tools-resources">Tools &amp; Resources</h2>
<p>NOA can invoke various tools through subordinate agents, including:
web research, code &amp; data analysis, file search, and automations. It
delegates model selection to ModelSelectorAgents and leverages
microservices to execute tasks. It works with the internal data plane
(OCI registry, MinIO, Postgres/pgvector, Supabase) to store and retrieve
artefacts, always within the trust boundary.# Board Agents — Executive
Team of ark‑os‑noa</p>
<h2 id="definition-role">Definition &amp; Role</h2>
<p>The <strong>Board Agents</strong> sit at the top of the
<strong>ark‑os‑noa</strong> organisation just below NOA. They are
analogous to an executive board in a company: each agent owns a domain
(strategy, operations, finance, legal, security, partnerships, research)
and has authority to commission <strong>MicroAgentStacks</strong> to
execute work. By design they are <em>few in number</em> but <em>broad in
scope</em>—their purpose is to translate NOA’s vision into specific
missions, ensure alignment with ElementArk/DeFlex’s business model, and
provide governance across all stacks and agents.</p>
<h2 id="roster-responsibilities">Roster &amp; Responsibilities</h2>
<ul>
<li><strong>Strategy/CTO Agent</strong> – Sets technical direction:
system architecture, Capsule (Full‑Illusion) adoption, environment
policies (no Docker‑in‑Docker), cohesion across services.</li>
<li><strong>COO Agent</strong> – Owns operational runbooks, SLAs,
scheduling and change management. Coordinates delivery timelines and
resource utilisation.</li>
<li><strong>CFO/FinOps Agent</strong> – Manages budgets and spend
telemetry. Optimises cost across compute, storage and model usage.</li>
<li><strong>Legal/Compliance Agent</strong> – Ensures licence
compliance, data governance, export controls and regulatory adherence.
Maintains policy frameworks.</li>
<li><strong>Security Agent</strong> – Enforces secrets management,
supply‑chain security, SBOM attestation and vulnerability thresholds.
Gatekeeper for risk.</li>
<li><strong>Growth/Partnerships Agent</strong> – Curates ingestion
roadmaps for repos, APIs and CRMs; drives ecosystem strategy and
partnership integrations.</li>
<li><strong>Digest Agent (R&amp;D)</strong> – Sits on the board as the
research arm. Its role is to <em>digest everything</em> (code, data,
SaaS, models) and surface insights. See <code>digest_agent.md</code> for
details.</li>
</ul>
<h2 id="operating-rules">Operating Rules</h2>
<ol type="1">
<li><strong>Delegation:</strong> Board Agents can spin up one or more
<strong>MicroAgentStacks</strong> to accomplish tasks. Each stack has
its own <strong>CommandChiefAgent</strong> orchestrating the details,
leaving the Board Agent to focus on strategy and oversight.</li>
<li><strong>Specialisation:</strong> When a task requires sophisticated
model selection, a Board Agent requests a
<strong>ModelSelectorAgent</strong> to choose the most appropriate AI
model or tool. This ensures tasks are executed with the right balance of
cost, latency and accuracy.</li>
<li><strong>Governance:</strong> Board Agents enforce policies across
stacks—licensing, vulnerability gates, security posture, and budget
limits. They maintain decision logs and risk registers for audit.</li>
<li><strong>Parallelism:</strong> Multiple stacks can run concurrently.
Board Agents schedule tasks to maximise throughput while respecting
resource constraints.</li>
</ol>
<h2 id="capabilities-1">Capabilities</h2>
<ul>
<li><strong>Multi‑project scheduling:</strong> assign and monitor
numerous tasks across different domains and stacks; handle dependencies
and deadlines.</li>
<li><strong>Cross‑repo initiatives:</strong> coordinate wide‑sweep
digest operations (e.g., SBOM/security posture across all repos) by
commissioning multiple stacks.</li>
<li><strong>Program governance:</strong> maintain an overarching view of
risks, mitigations, budget spend, and deliverable quality.</li>
<li><strong>Policy enforcement:</strong> integrate security scanners,
licence gates, and compliance checks into the workflow.</li>
</ul>
<h2 id="tools-signals">Tools &amp; Signals</h2>
<p>Board Agents interact with the system through:</p>
<ul>
<li><strong>Research &amp; analysis tools:</strong> for web search, code
parsing and data exploration within the current year’s context.</li>
<li><strong>Change control &amp; telemetry:</strong> CI/CD gates, policy
engines (e.g. OPA), vulnerability scanners and cost dashboards.</li>
<li><strong>Observability feeds:</strong> real‑time traces, metrics and
logs aggregated from MicroAgentStacks and sidecars. These signals inform
decisions on scaling up/down stacks or raising alerts.</li>
</ul>
<h2 id="relationship-to-other-components">Relationship to Other
Components</h2>
<ul>
<li><strong>NOA:</strong> Board Agents receive missions from NOA and
report status back. They provide domain expertise and enforce governance
while letting NOA handle high‑level planning and cross‑domain
coordination.</li>
<li><strong>MicroAgentStacks:</strong> Board Agents are the owners of
stacks. They commission stacks to achieve defined objectives and
decommission them when tasks complete. Each stack operates autonomously
but reports progress to its Board Agent.</li>
<li><strong>ModelSelectorAgents:</strong> When tasks require AI model
inference, Board Agents request a ModelSelector to choose among local or
hosted models. The selection is recorded in the trace for audit.</li>
<li><strong>Digest Agent:</strong> The Digest Agent is part of the Board
but behaves like an R&amp;D lab—collecting raw information, synthesising
knowledge graphs and summarising findings for the board to act on.</li>
</ul>
<p>By keeping the Board Agents separate from execution details yet close
enough to enforce policy, ark‑os‑noa achieves a balance between
<strong>strategic oversight</strong> and <strong>operational
agility</strong>. # ModelSelectorAgents — Choosing the Right Tool for
the Job</p>
<h2 id="purpose">Purpose</h2>
<p>A <strong>ModelSelectorAgent</strong> specialises in selecting the
best AI model or tool for a given task. In the context of ark‑os‑noa,
tasks vary widely—from reasoning and planning, to code analysis, to data
transformation. Selecting the wrong model can waste resources or
compromise privacy. The ModelSelector provides an intelligent
arbitration layer, helping Board Agents and
<strong>MicroAgentStacks</strong> achieve high quality results while
respecting cost, latency and privacy constraints.</p>
<h2 id="framework-1">Framework</h2>
<ul>
<li><strong>Inputs:</strong> Each call to a ModelSelector includes a
task description, input size (e.g. document length, number of files),
the privacy tier (public, sensitive, confidential), latency budget, and
a cost cap. These parameters come from the requesting agent (often a
Board Agent or CommandChiefAgent).</li>
<li><strong>Decision Graph:</strong> The ModelSelector applies a
decision graph:
<ol type="1">
<li><strong>Task classification</strong> – Is this reasoning/planning,
bulk transformation, code/data manipulation, or something else?</li>
<li><strong>Complexity estimation</strong> – How large or intricate is
the input? This influences whether to use a bigger model or a
lightweight one.</li>
<li><strong>Model/Tool selection</strong> – Choose from a catalogue of
available models (remote APIs, local models served via llama.cpp/Ollama,
code runners, data converters) using heuristics or learned
policies.</li>
<li><strong>Guardrails assertion</strong> – Check licensing, privacy
levels and security requirements. For example, confidential data must
stay on‑prem and use local models.</li>
</ol></li>
<li><strong>Outputs:</strong> A plan specifying the chosen model or
tool, the expected cost/latency, and a rationale. The rationale becomes
part of the execution <strong>Trace</strong>, enabling auditing and
future optimisation.</li>
</ul>
<h2 id="default-policy">Default Policy</h2>
<p>The default policy can be tuned, but common guidelines include:</p>
<ol type="1">
<li><strong>Reasoning / Planning tasks:</strong> Use high‑quality
generalist models (e.g. GPT‑5). These tasks benefit from advanced
reasoning and tolerance for slower latency when results matter.</li>
<li><strong>Bulk transforms / formatting:</strong> Use fast,
cost‑efficient models; they handle repetitive conversions without
needing deep reasoning.</li>
<li><strong>Code &amp; data tasks:</strong> Prefer dedicated code
analysis tools or local runtimes for safety. Use sandboxed execution to
evaluate code or parse data. Employ smaller codex models when
summarising code.</li>
<li><strong>Offline/local fallbacks:</strong> If the privacy tier
demands on‑prem processing or if network latency is unacceptable, use
local models served via llama.cpp, vLLM or similar frameworks. This
reduces latency and eliminates external data exposure.</li>
</ol>
<h2 id="tools-telemetry">Tools &amp; Telemetry</h2>
<ul>
<li><strong>Model catalogues:</strong> The selector maintains metadata
about available models—accuracy, context limits, token costs, latency
benchmarks, licensing and hardware requirements. It syncs with the local
model server and remote provider APIs.</li>
<li><strong>Cost/latency forecaster:</strong> Predicts cost and latency
using historical telemetry and dynamic system load. This helps decide
when to use a cheaper but slower model vs. a more expensive
high‑performance one.</li>
<li><strong>Performance feedback:</strong> The selector ingests feedback
after tasks complete (e.g. success, error rate, user satisfaction). Over
time it learns to better match tasks to models.</li>
</ul>
<h2 id="relationship-to-other-components-1">Relationship to Other
Components</h2>
<ul>
<li><strong>Board Agents:</strong> Request ModelSelector assistance when
their tasks involve AI/ML. They set budgets and specify privacy tiers.
The ModelSelector returns a plan and rationale.</li>
<li><strong>MicroAgentStacks:</strong> CommandChiefAgents invoke
ModelSelectors inside their stacks when a task requires AI processing.
This ensures each stack uses consistent policies and optimal
models.</li>
<li><strong>NOA:</strong> Maintains overarching policies for model
selection (allowed licences, vulnerability gates, GPU quotas). The
ModelSelector enforces these policies and logs decisions back to NOA’s
audit trail.</li>
</ul>
<h2 id="benefits">Benefits</h2>
<ul>
<li><strong>Efficiency:</strong> Avoids blindly calling the largest or
default model for every task, saving compute and cost.</li>
<li><strong>Compliance:</strong> Ensures tasks adhere to privacy and
licensing requirements—confidential data stays internal.</li>
<li><strong>Transparency:</strong> Provides a clear rationale for each
selection so decisions can be audited and improved.</li>
<li><strong>Extensibility:</strong> New models or tools can be added to
the catalogue; the decision graph can be refined with new criteria or
learned policies.</li>
</ul>
<p>By delegating model/tool choice to a dedicated ModelSelectorAgent,
ark‑os‑noa keeps business logic and AI expertise separate, resulting in
better outcomes and traceable decisions. # MicroAgentStack — Cooperative
Work Pods</p>
<h2 id="definition">Definition</h2>
<p>A <strong>MicroAgentStack</strong> is a deployable cluster of
cooperative agents assembled to accomplish a bounded objective. Think of
it as a project team spun up on demand: each stack has its own
<strong>CommandChiefAgent</strong> (the stack master), a set of
specialised Operators, Adapters and Guards, and a dedicated workspace.
Stacks can be created, scaled and destroyed rapidly, making them the
primary execution units within ark‑os‑noa.</p>
<h2 id="composition">Composition</h2>
<ul>
<li><strong>CommandChiefAgent (Stack Master):</strong> Orchestrates the
stack, decomposes tasks, assigns work to subordinate agents, monitors
progress, resolves conflicts and enforces SLAs.</li>
<li><strong>Operators:</strong> Specialised agents that perform specific
functions. Examples include code runners (execute code), data wranglers
(transform data), doc generators (produce reports), testers (run
unit/integration tests) and packagers (build zips, PDFs).</li>
<li><strong>Adapters:</strong> Connectors to external systems (repos,
CRMs, APIs) and publishers to internal services (registry, MinIO,
Postgres). Adapters abstract away details like auth and
rate‑limits.</li>
<li><strong>Guards:</strong> Policy enforcement points—security
scanners, licence checkers, quality gates. They ensure the stack adheres
to policies defined by NOA and the Board Agents.</li>
</ul>
<h2 id="goals-1">Goals</h2>
<ol type="1">
<li><strong>Deliver end‑to‑end outcomes:</strong> A stack should own the
entire life cycle of its objective—from cloning a repo to producing a
digest report, from running tests to publishing a package.</li>
<li><strong>Scale horizontally:</strong> Multiple stacks can be spun up
concurrently when tasks are independent or parallelisable. This enables
large scale operations like digesting hundreds of repos
simultaneously.</li>
<li><strong>Clean teardown:</strong> After completion, a stack cleans up
its resources (containers, temporary volumes) and archives logs, SBOMs
and artefacts with proper retention policies.</li>
</ol>
<h2 id="lifecycle-1">Lifecycle</h2>
<ol type="1">
<li><strong>Bootstrap:</strong> Given inputs (e.g. repo URL, CRM base
URL, model list), the CommandChiefAgent creates a
<strong>WorkPlan</strong>, prepares the environment and mounts necessary
sidecars. It avoids Docker‑in‑Docker by using <strong>Capsule</strong>
sidecars to talk to the outer BuildKit/containerd
environment【43537238352704†L1068-L1088】.</li>
<li><strong>Execute:</strong> The stack runs its Operators in parallel
where possible. Retrying tasks with exponential backoff ensures
resilience; failures trigger controlled retries or escalation to the
Board Agent.</li>
<li><strong>Validate:</strong> Once tasks finish, Guards run acceptance
tests (e.g. unit tests, SBOM scans, licence checks) and produce
human‑readable summaries. If acceptance criteria fail, the stack either
retries or fails the WorkPlan.</li>
<li><strong>Package:</strong> On success, the stack assembles outputs
into deliverables (zip file, compiled PDF, JSON indices). It updates
internal registries (OCI images, Postgres metadata, vector DB) and
publishes logs and traces.</li>
<li><strong>Archive:</strong> The stack removes its runtime environment
and persists all logs, SBOMs, run IDs, and checksums. Retention policies
decide how long to keep each artefact.</li>
</ol>
<h2 id="oneliners-conventions">One‑liners &amp; Conventions</h2>
<ul>
<li>Stacks are named by timestamps or descriptive identifiers
(e.g. <code>stack‑20250822‑103045</code>).</li>
<li>They maintain their own directory structure (<code>in/</code>,
<code>work/</code>, <code>out/</code>, <code>logs/</code>) for clarity
and reproducibility.</li>
<li>Each stack produces a unique run ID and attaches it to all outputs
and logs for traceability.</li>
</ul>
<h2 id="relationship-to-other-components-2">Relationship to Other
Components</h2>
<ul>
<li><strong>Board Agents:</strong> Create and oversee stacks. Each stack
reports to its Board Agent. Board Agents can run multiple stacks in
parallel.</li>
<li><strong>ModelSelectorAgents:</strong> When a stack requires AI
processing, the CommandChiefAgent requests a ModelSelector to choose the
appropriate model and logs the rationale.</li>
<li><strong>Digest Agent:</strong> Often uses MicroAgentStacks to
perform large‑scale digestions across many repos or datasets. Each stack
digests one or more sources and returns results to the Digest
Agent.</li>
</ul>
<p>MicroAgentStacks bring structure, scalability and reliability to
ark‑os‑noa’s execution model. By isolating work into bounded pods, the
system can handle complex, parallel workflows without turning into a
monolith. # Digest Agent — R&amp;D Engine for ark‑os‑noa</p>
<h2 id="role-position">Role &amp; Position</h2>
<p>The <strong>Digest Agent</strong> operates as the research and
development arm of the Board Agents. Its primary mission is to
<em>“digest everything”</em>—code repositories, datasets, documents,
APIs, SaaS systems (including live CRMs) and even AI models. By
analysing these sources, the Digest Agent extracts structured knowledge,
builds semantic indices, and surfaces insights that inform strategic
decisions. Though part of the Board, it behaves like a self‑contained
lab, spinning up <strong>MicroAgentStacks</strong> to perform
large‑scale digestions.</p>
<h2 id="pipeline">Pipeline</h2>
<ol type="1">
<li><strong>Discover:</strong> Identify sources to digest. This includes
scanning internal GitHub repos, listing connected APIs/CRMs, and reading
the current model ingestion list. Discovery may rely on board directives
or scheduled tasks.</li>
<li><strong>Fetch:</strong> Clone or synchronise the source material.
For code repos, perform a shallow clone and gather dependency lock
files. For CRMs or APIs, pull metadata and sample records while
respecting rate limits. Handle authentication using secure tokens from
the secrets manager.</li>
<li><strong>Parse:</strong> Use language‑specific parsers (Python AST,
ts‑morph for JS/TS, go/ast, Rust syn, JavaParser) to analyse code and
extract modules, functions, classes and call graphs. For API schemas,
parse OpenAPI/GraphQL definitions. Build an <strong>SBOM</strong> to
capture all packages and versions.</li>
<li><strong>Analyze:</strong> Generate embeddings for code,
documentation and data using models selected via the
<strong>ModelSelectorAgent</strong>. Build a <strong>knowledge
graph</strong> linking functions, data structures, APIs and entities.
Identify external API calls, config surfaces and extension points. Apply
entity linking to unify references across sources.</li>
<li><strong>Summarize:</strong> Produce layered summaries: per file, per
module, per repository and across repositories. Summaries highlight the
system’s purpose, architecture, dependencies, risks and extension
points. The Digest Agent uses LLMs to craft human‑readable reports and
cross‑links to original sources.</li>
<li><strong>Surface:</strong> Publish outputs as markdown dossiers,
dashboards and vector DB upserts. Persist <code>profile.json</code>,
<code>system_card.md</code>, <code>kg.json</code>, and embeddings. Offer
search and retrieval APIs for downstream agents.</li>
<li><strong>Secure:</strong> Scan for secrets and vulnerabilities using
tools like Trivy, Grype and Gitleaks. Classify findings by severity and
quarantine sensitive information. Tag licences and export‑control
flags【43537238352704†L1068-L1088】.</li>
</ol>
<h2 id="tools">Tools</h2>
<ul>
<li><strong>Web research:</strong> limited to current‑year sources,
retrieving official documentation and examples.</li>
<li><strong>Language parsers &amp; AST tools:</strong> Python’s
<code>ast</code>, TS’s <code>ts‑morph</code>, Go’s <code>go/ast</code>,
Rust’s <code>syn</code>, Java’s <code>JavaParser</code>.</li>
<li><strong>Security scanners:</strong> Syft to produce SBOMs; Grype and
Trivy to scan for vulnerabilities; Gitleaks to detect secrets; Semgrep
for static analysis.</li>
<li><strong>Embeddings &amp; vector DB:</strong> Sentence transformers
or llama.cpp embedding models; pgvector or Qdrant to store vectors and
link them to original files.</li>
<li><strong>Visualization &amp; reports:</strong> Graph builders,
markdown generators and PDF compilers.</li>
</ul>
<h2 id="outputs">Outputs</h2>
<p>The Digest Agent delivers:</p>
<ul>
<li><strong>Digest reports:</strong> Markdown documents
(e.g. <code>2025‑08‑22_digest_report.md</code>) summarising
findings.</li>
<li><strong>Structured indices:</strong> JSONL files representing the
knowledge graph, call graph and embedding metadata. These feed search
and retrieval APIs.</li>
<li><strong>SBOM &amp; security reports:</strong> Comprehensive lists of
dependencies and vulnerabilities.</li>
<li><strong>Vector store entries:</strong> Embeddings upserted to the
chosen vector DB for semantic search.</li>
</ul>
<h2 id="relationship-to-other-components-3">Relationship to Other
Components</h2>
<ul>
<li><strong>Board Agents:</strong> Commission digestion tasks and
consume the Digest Agent’s findings when making strategic
decisions.</li>
<li><strong>MicroAgentStacks:</strong> Used to parallelise large
digests—each stack handles a set of sources and feeds results back to
the Digest Agent.</li>
<li><strong>ModelSelectorAgents:</strong> Select embedding models and
summarisation LLMs appropriate for each source type. For example, code
summarisation may use a codex model, while plain text summarisation uses
a general LLM.</li>
<li><strong>Data &amp; Storage layer:</strong> Stores artefacts and
indices in MinIO, Postgres and the vector store. The Digest Agent
ensures proper metadata tagging and retention policies.</li>
</ul>
<p>By systematically consuming and analysing every relevant piece of
information, the Digest Agent turns unstructured data into actionable
knowledge for ark‑os‑noa’s decision makers. # Backend — Services &amp;
Infrastructure of ark‑os‑noa</p>
<h2 id="purpose-1">Purpose</h2>
<p>The <strong>backend</strong> of ark‑os‑noa comprises all of the
runtime services and infrastructure that turn high‑level plans into
concrete work. It includes the event bus, microservices that implement
the <strong>Expanded Digest Pipeline</strong>, sidecars that enable the
<strong>Capsule</strong> pattern, and internal data stores. Together,
these components provide a robust, scalable and secure environment for
executing tasks, orchestrated by NOA and the Board Agents.</p>
<h2 id="services-microservices">Services &amp; Microservices</h2>
<h3 id="core-pipeline-services">Core Pipeline Services</h3>
<p>The digest‑everything pipeline is decomposed into a series of
microservices, each responsible for a discrete stage. Running them as
independent services ensures that each can scale, fail and be updated
independently, which is aligned with microservice best
practices【43537238352704†L1068-L1088】.</p>
<ol type="1">
<li><strong>Intake Service:</strong> Receives digest requests; validates
inputs (repo URLs, API endpoints, model lists); creates provenance
records and initializes workspace directories.</li>
<li><strong>Classifier Service:</strong> Detects programming languages,
build systems, service types (CLI, API, library) and licences. Produces
a <code>profile.json</code> summarising the source.</li>
<li><strong>Graph Extract Service:</strong> Parses code and schemas to
build call graphs, data flow graphs and config surfaces. Supports
multi‑language parsing (Python, JS/TS, Go, Rust, Java). Outputs
<code>kg.json</code> and <code>system_card.md</code>.</li>
<li><strong>Embeddings Service:</strong> Generates embeddings for code
and documentation using models selected by ModelSelectorAgents. Upserts
vectors to pgvector or Qdrant.</li>
<li><strong>Env Synthesis Service:</strong> Emits Dockerfiles,
docker‑compose YAML, Kubernetes manifests, <code>.env.example</code>,
<code>Makefile</code> targets and config schemas. Ensures reproducible
builds using outer BuildKit (no DinD).</li>
<li><strong>Safety Service:</strong> Runs SBOM generation (Syft),
vulnerability scans (Grype/Trivy), secret scans (Gitleaks) and static
analysis (Semgrep). Applies policy gates; stops the pipeline on critical
issues.</li>
<li><strong>Runner Service:</strong> Builds and runs the source in a
controlled container environment; executes existing tests or generates
smoke tests; produces <code>demo.md</code>.</li>
<li><strong>Integrator Service:</strong> Generates adapters (SDKs for
Python, Node, Go), telemetry hooks and policy stubs; prepares packaging
instructions.</li>
<li><strong>Registrar Service:</strong> Writes outputs and metadata to
storage (registry, MinIO, Postgres); registers embeddings; updates
indexes for search.</li>
</ol>
<h3 id="auxiliary-services">Auxiliary Services</h3>
<ul>
<li><strong>CRM Strangler Proxy:</strong> Provides a transparent layer
between internal clients and an external CRM. It records
requests/responses, supports <em>shadow</em> and <em>write‑through</em>
modes, and allows incremental internal re‑implementation of CRM
features.</li>
<li><strong>Model Serving:</strong> Hosts local models using frameworks
like llama.cpp, Ollama or vLLM. Exposes endpoints for inference and
embedding generation. Each model server is packaged in its own container
with health checks.</li>
<li><strong>Gateway API:</strong> A FastAPI service exposing endpoints:
<code>/digest</code>, <code>/capsule/spawn</code>,
<code>/crm/toggle</code>, <code>/models/ingest</code>,
<code>/models/benchmark</code>, <code>/admin/*</code>. Acts as the
single entry point for external clients and the front‑end.</li>
</ul>
<h2 id="event-bus-orchestration">Event Bus &amp; Orchestration</h2>
<ul>
<li><strong>Redis Streams:</strong> Provides the primary event bus for
inter‑service communication. Services consume and produce events in a
decoupled fashion. The bus also supports message persistence and
backpressure.</li>
<li><strong>NATS (optional):</strong> A lightweight publish/subscribe
system for high fan‑out or cross‑cluster communication. Enabled via a
feature flag.</li>
<li><strong>Workflow Engine:</strong> A simple DAG engine built on Redis
to coordinate pipeline tasks with retries and backoff. Temporal or Argo
Workflows can be integrated later for more sophisticated
orchestrations.</li>
</ul>
<h2 id="capsule-sidecars">Capsule Sidecars</h2>
<p>All containers run inside a “Capsule” to simulate
container‑in‑container and Kubernetes‑in‑Kubernetes workflows without
the security and performance drawbacks【716409907369096†L1037-L1067】.
Capsule sidecars include:</p>
<ol type="1">
<li><strong>Build‑Proxy:</strong> A lightweight service that proxies
inner <code>docker build</code> and <code>nerdctl</code> commands to the
outer BuildKit daemon. It exposes a local socket inside the Capsule but
forwards build requests externally, avoiding duplicate layer
storage.</li>
<li><strong>Service‑Mirror:</strong> Watches inner service definitions
and publishes corresponding services in the outer service mesh with mTLS
and SLO configurations. This allows inner services to be reachable and
observable from the outer plane.</li>
<li><strong>Policy Agent (OPA):</strong> Enforces egress rules, resource
quotas, and other policies at the Capsule boundary. It integrates with
eBPF to block unauthorised traffic.</li>
<li><strong>Telemetry Agent:</strong> Collects traces, metrics and logs
from the inner services and sidecars. It forwards data to the central
observability stack with proper trace‑ID propagation.</li>
<li><strong>vcluster (optional):</strong> Provides a lightweight
Kubernetes API server inside the Capsule for tools that require kubectl.
It maps pods to the parent cluster’s nodes without duplicating container
runtimes.</li>
</ol>
<h2 id="data-stores">Data Stores</h2>
<ul>
<li><strong>OCI Registry:</strong> Stores container images, compiled
outputs and Capsule definitions. The registry uses content‑addressed
storage and enforces immutable tags.</li>
<li><strong>MinIO:</strong> Stores large artefacts, zipped deliverables,
SBOMs and data sets. Supports versioning and server‑side
encryption.</li>
<li><strong>Postgres (+ Supabase):</strong> Maintains metadata
(profiles, system cards, run logs), traces, job statuses and vector
search indices. Supabase provides developer APIs and pgvector
integration.</li>
<li><strong>Vector Store:</strong> For embeddings. The backend can be
<code>pgvector</code> in Postgres or an external Qdrant instance. A
feature flag chooses which driver to enable.</li>
</ul>
<h2 id="security-compliance">Security &amp; Compliance</h2>
<p>The backend enforces numerous policies:</p>
<ul>
<li><strong>No DinD:</strong> Build operations are forwarded to outer
BuildKit/containerd; containers run with user namespaces and seccomp,
preventing container‑root escalation【43537238352704†L1068-L1088】.</li>
<li><strong>Licence &amp; vulnerability gates:</strong> The Safety
service halts builds on critical issues; the Board Agents define
accepted licence lists and vulnerability thresholds.</li>
<li><strong>Secrets management:</strong> Secrets are never stored in
environment variables. They are mounted as files via Vault or similar
systems, and sidecars are responsible for retrieving them.</li>
<li><strong>Audit trails:</strong> Every API call, pipeline event and
model selection decision logs context (who, what, when, rationale).
These logs live in Postgres and are tied to run IDs.</li>
</ul>
<h2 id="development-testing">Development &amp; Testing</h2>
<ul>
<li><strong>Makefile:</strong> Provides convenience targets
(<code>make up</code>, <code>make down</code>, <code>make logs</code>,
<code>make demo</code>, <code>make scan</code>,
<code>make lock-images</code>) for developers. It ensures consistent
environment setup and teardown.</li>
<li><strong>Docker‑Compose:</strong> Defines services and dependencies;
profiles enable optional components like NATS, Supabase and vcluster.
Compose is used for local development. For production, manifests under
<code>k8s/</code> can be applied to a Kubernetes cluster.</li>
<li><strong>Automated tests:</strong> Unit and integration tests run
within the Runner Service; security scanners run in the Safety Service.
CI pipelines (to be implemented post‑launch) build images, run tests,
generate SBOMs and publish artefacts.</li>
</ul>
<p>By modularising the backend into clear services and infrastructure
layers, ark‑os‑noa achieves the flexibility of microservices with the
discipline of reproducible builds and strong security controls. # Data
&amp; Storage — Securing the ark‑os‑noa Data Plane</p>
<h2 id="principle">Principle</h2>
<p>The data layer of ark‑os‑noa is built around a core principle:
<strong>keep all storage within the trust boundary</strong>. All
artefacts—images, datasets, logs, metadata, SBOMs—are retained
internally, signed and versioned. Only signed, approved deliverables are
exported. This ensures confidentiality, integrity and provenance across
the platform.</p>
<h2 id="components">Components</h2>
<ol type="1">
<li><strong>Private OCI Registry:</strong> Hosts container images,
Capsule definitions, build outputs and adapters. Using a private
registry prevents untrusted images from entering the environment and
allows BuildKit to push/pull from a controlled backend.</li>
<li><strong>MinIO (S3‑compatible object store):</strong> Serves as the
main artefact store. It holds large files (zip archives, compiled PDFs),
dataset fragments, SBOM documents, vulnerability reports and even model
shards. MinIO offers versioning, server‑side encryption (SSE) and
lifecycle management.</li>
<li><strong>Postgres:</strong> Stores structured metadata: run logs,
profiles (<code>profile.json</code>), system cards
(<code>system_card.md</code>), call graphs (<code>kg.json</code>), job
statuses, policy decisions and audit trails. Postgres also stores vector
embeddings via the <code>pgvector</code> extension.</li>
<li><strong>Supabase:</strong> A self‑hosted instance of Supabase
augments Postgres with developer APIs, authentication and real‑time
features. It provides a convenient interface for front‑end applications
and external tools until the platform fully internalises these
capabilities.</li>
<li><strong>Vector Store:</strong> Embeddings generated by the
Embeddings Service are stored in either <code>pgvector</code> (Postgres)
or a dedicated Qdrant cluster. pgvector is enabled by default for
simplicity; Qdrant can be turned on via a feature flag to support larger
vector workloads.</li>
</ol>
<h2 id="policies-best-practices">Policies &amp; Best Practices</h2>
<ul>
<li><strong>Immutability:</strong> Artefacts are stored
content‑addressed using SHA‑256 digests. Tags or names are pointers to
immutable content; rewriting tags triggers new versions. This prevents
tampering and ensures reproducible builds.</li>
<li><strong>Lineage &amp; Provenance:</strong> Each deliverable (zip,
PDF, image) links back to its inputs, tools, models and run ID. Build
provenance is stored as JSON attestation, capturing the environment,
command, dependency versions and commit hashes.</li>
<li><strong>Retention:</strong> Short‑lived runs (e.g. experimental
digests) are kept for a limited period; long‑term artefacts
(e.g. official releases) are retained indefinitely. Policies can be
configured per project or domain.</li>
<li><strong>Access Control:</strong> The data plane uses least
privilege. Microservices receive temporary scoped tokens to access the
object store or registry; access is audited. Secrets (e.g. tokens, keys)
are stored in a secrets manager (Vault) and mounted as files, never as
environment variables.</li>
</ul>
<h2 id="integration-with-other-components">Integration with Other
Components</h2>
<ul>
<li><strong>Backend services:</strong> Interact with the registry and
MinIO via signed URLs or direct API calls. BuildKit pushes images to the
registry; the Registrar Service writes artefacts to MinIO and records
metadata in Postgres.</li>
<li><strong>Digest Agent:</strong> Reads and writes to MinIO and
Postgres; uploads embeddings to the vector store. It uses the registry
to store intermediate build images.</li>
<li><strong>Model Selector and Model Servers:</strong> Use Postgres (via
pgvector or Qdrant) to store model metadata and evaluation results.
Models themselves may be stored as OCI artefacts or in MinIO
shards.</li>
<li><strong>Front‑end:</strong> Accesses Supabase for real‑time updates
and uses signed URLs to fetch artefacts from MinIO.</li>
</ul>
<h2 id="oneliners-conventions-1">One‑Liners &amp; Conventions</h2>
<div class="sourceCode" id="cb1"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create local directories mirroring services (for dev/testing)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> <span class="at">-p</span> storage/<span class="dt">{oci</span><span class="op">,</span><span class="dt">minio</span><span class="op">,</span><span class="dt">postgres</span><span class="op">,</span><span class="dt">supabase</span><span class="op">,</span><span class="dt">artifacts}</span> <span class="kw">&amp;&amp;</span> <span class="ex">tree</span> <span class="at">-L</span> 2 storage <span class="kw">||</span> <span class="fu">ls</span> <span class="at">-R</span> storage</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Content‑address an artefact</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="va">digest</span><span class="op">=</span><span class="va">$(</span><span class="fu">sha256sum</span> output.zip <span class="kw">|</span> <span class="fu">awk</span> <span class="st">&#39;{print $1}&#39;</span><span class="va">)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cp</span> output.zip storage/artifacts/<span class="va">${digest}</span>.zip</span></code></pre></div>
<h2 id="why-internal-data-planes-matter">Why Internal Data Planes
Matter</h2>
<p>Keeping storage internal reduces the attack surface and simplifies
compliance. Data never leaves the environment without explicit signing
and approval. When combined with provenance tracking, this approach
ensures that every piece of data can be traced back to its origin and
verified—critical for regulated environments and supply‑chain integrity.
# Combined Framework &amp; Architecture of ark‑os‑noa</p>
<h2 id="highlevel-overview">High‑Level Overview</h2>
<p><strong>ark‑os‑noa</strong> is an <strong>agentic AI
platform</strong> designed to realise ElementArk/DeFlex’s business
model. It combines hierarchical organisational patterns (NOA → Board
Agents → MicroAgentStacks → microservices) with modern infrastructure
techniques (Capsule/Full‑Illusion pattern, private data plane, event
bus) and an adaptable AI layer (ModelSelectorAgents and Digest Agent).
The result is a <strong>“hive mind”</strong> of specialised agents
capable of digesting, reasoning about and producing artefacts across
software, data and SaaS systems.</p>
<h2 id="layers-hierarchy">Layers &amp; Hierarchy</h2>
<h3 id="strategy-orchestration-layer">1. Strategy &amp; Orchestration
Layer</h3>
<ul>
<li><strong>NOA:</strong> The ExecutiveCommanderChiefAgent at the top.
Transforms business goals into actionable plans, assigns Board Agents,
sets policies, and monitors execution.</li>
<li><strong>Board Agents:</strong> Domain‑specific executives
(Strategy/CTO, COO, CFO/FinOps, Legal/Compliance, Security,
Growth/Partnerships, Digest). Each can commission work via
MicroAgentStacks and request ModelSelector assistance.</li>
</ul>
<h3 id="execution-layer">2. Execution Layer</h3>
<ul>
<li><strong>MicroAgentStacks:</strong> On‑demand work pods orchestrated
by a CommandChiefAgent. Each stack contains Operators, Adapters and
Guards and runs through a defined lifecycle (Bootstrap → Execute →
Validate → Package → Archive). Stacks interact with external sources
(repos, CRMs, APIs) and internal services via Adapters.</li>
<li><strong>Expanded Digest Pipeline:</strong> A set of microservices
(Intake, Classifier, Graph Extract, Embeddings, Env Synthesis, Safety,
Runner, Integrator, Registrar) that perform the actual work. Each is
loosely coupled via an event bus and runs inside the Capsule
environment. CRM Strangler and Model Serving are additional
services.</li>
</ul>
<h3 id="infrastructure-layer">3. Infrastructure Layer</h3>
<ul>
<li><strong>Capsule Architecture (Full Illusion):</strong> Encapsulates
stacks and services in a sandbox that forwards build operations and
network traffic to the outer runtime. Capsule sidecars (Build‑Proxy,
Service‑Mirror, Policy Agent, Telemetry Agent, optionally vcluster)
provide the illusion of Docker‑in‑Docker and Kubernetes‑in‑Kubernetes
without their drawbacks【716409907369096†L1037-L1067】.</li>
<li><strong>Event Bus &amp; Orchestration:</strong> Redis Streams
(primary) and optional NATS enable asynchronous communication. A
workflow engine coordinates the pipeline steps, handling retries and
backoff.</li>
<li><strong>Data Plane:</strong> Private OCI registry, MinIO, Postgres
(+ pgvector/Supabase) and optionally Qdrant. This plane stores
everything from container images to embeddings and ensures data stays
within the trust boundary.</li>
<li><strong>Observability &amp; Security:</strong> OTel tracing,
Prometheus metrics, policy agents, SBOM/vulnerability scanners and
secrets management. The <strong>no DinD</strong> policy and user
namespaces reduce privilege escalation
risk【43537238352704†L1068-L1088】.</li>
</ul>
<h2 id="how-the-pieces-fit-together">How the Pieces Fit Together</h2>
<ol type="1">
<li><strong>Goal Intake:</strong> A high‑level goal arrives. NOA
normalises it into a WorkPlan and determines which Board Agents are
responsible.</li>
<li><strong>Board Planning:</strong> Board Agents refine the goal,
assign budgets, define SLAs and set policies. They request
MicroAgentStacks and ModelSelectorAgents as needed.</li>
<li><strong>Stack Deployment:</strong> For each task, a MicroAgentStack
is spawned. The stack uses Adapters to fetch sources (repos, CRMs),
Operators to parse/analyse, and Guards to enforce policies.
Microservices implement the digest pipeline, orchestrated via the event
bus.</li>
<li><strong>Model Selection &amp; Execution:</strong> When a service or
operator needs AI inference (embeddings, summarisation, code
explanation), it calls a ModelSelectorAgent. The selected model is
executed via local model servers or remote APIs.</li>
<li><strong>Data Persistence:</strong> Outputs from each step (SBOMs,
graphs, embeddings, demos) are persisted via the Data Plane. The
Registrar Service updates indexes and metadata.</li>
<li><strong>Completion &amp; Reporting:</strong> Once tasks finish, the
stack packages results into a zip and compiled PDF, publishes them to
MinIO and the registry, and updates Postgres. NOA receives a report and
archives the run.</li>
</ol>
<h2 id="why-this-architecture">Why This Architecture?</h2>
<ol type="1">
<li><strong>Modularity &amp; Scalability:</strong> By decomposing
functionality into microservices and agents, ark‑os‑noa can scale
horizontally and update components independently—avoiding the pitfalls
of monolithic systems【43537238352704†L1068-L1088】.</li>
<li><strong>Security &amp; Compliance:</strong> The Capsule pattern, no
DinD policy, private data plane and sidecar enforcement minimise the
attack surface. SBOMs, licences and vulnerability scans ensure
supply‑chain integrity.</li>
<li><strong>Intelligence &amp; Adaptability:</strong>
ModelSelectorAgents enable adaptive AI usage; the Digest Agent builds
knowledge graphs and embeddings; the board can ingest CRMs and SaaS
systems without downtime using the strangler proxy.</li>
<li><strong>Auditability &amp; Provenance:</strong> Every decision,
model selection and action is logged in Postgres and associated with a
run ID. Artefacts are content‑addressed and signed. This supports
post‑mortems, compliance and future learning.</li>
</ol>
<h2 id="extensibility">Extensibility</h2>
<ul>
<li><strong>New Board roles:</strong> Additional executives
(e.g. Marketing, HR) can be added by extending the roster and defining
their domains and policies.</li>
<li><strong>Additional microservices:</strong> New processing stages
(e.g. code transformers, simulation engines) can be plugged into the
pipeline without redesigning the whole system.</li>
<li><strong>Hybrid deployment:</strong> While Compose is used locally,
Kubernetes manifests (<code>k8s/</code>) can be applied to a cluster;
the same Capsule pattern applies.</li>
<li><strong>Model &amp; Connector expansions:</strong> New AI models are
registered via the ModelSelector; new connectors are implemented by
Adapters to integrate more SaaS or data sources.</li>
</ul>
<p>The <strong>Combined Framework &amp; Architecture</strong> unifies
strategic planning, microservice execution, security and AI into a
cohesive system. It is intentionally modular to allow continuous growth
and improvement. # API, Connectors &amp; Front‑End of ark‑os‑noa</p>
<h2 id="gateway-api">Gateway API</h2>
<p>The <strong>Gateway API</strong> is the central entry point for
interacting with ark‑os‑noa’s backend services. Implemented using
FastAPI, it exposes endpoints for ingesting sources, spawning capsules,
toggling CRM behaviours, ingesting models and administering the
system.</p>
<h3 id="key-endpoints">Key Endpoints</h3>
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 26%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr class="header">
<th>Endpoint</th>
<th>Method</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>/digest</code></td>
<td>POST</td>
<td>Submit a digest request. The request includes sources (e.g. repo
URL, API base URL), intent (integrate, analyse), and optional metadata.
It triggers the Intake Service and returns a job ID.</td>
</tr>
<tr class="even">
<td><code>/capsule/spawn</code></td>
<td>POST</td>
<td>Spawn a new Capsule environment. Returns Capsule identifiers and
access tokens. Used when custom stacks need to be run manually or via
the front‑end.</td>
</tr>
<tr class="odd">
<td><code>/crm/toggle</code></td>
<td>POST</td>
<td>Toggle the CRM Strangler Proxy mode for a specific endpoint
(e.g. enable write‑through for <code>/contacts</code>). Allows
incremental migration from external CRM to internal implementation.</td>
</tr>
<tr class="even">
<td><code>/models/ingest</code></td>
<td>POST</td>
<td>Add a model to the local registry. Accepts a model identifier
(e.g. Hugging Face repo) and optional metadata. The Model Serving
service pulls the model and makes it available through the
ModelSelector.</td>
</tr>
<tr class="odd">
<td><code>/models/benchmark</code></td>
<td>POST</td>
<td>Run evaluations on local or remote models. Returns latency, cost and
accuracy metrics that feed into the ModelSelector’s decision graph.</td>
</tr>
<tr class="even">
<td><code>/admin/*</code></td>
<td>GET/POST</td>
<td>Administrative endpoints for tasks such as inspecting job statuses,
viewing SBOMs, retrieving logs, enabling/disabling features (NATS,
Supabase, vcluster) and rotating secrets. Protected via authentication
and authorisation.</td>
</tr>
</tbody>
</table>
<p>All endpoints accept and return JSON; error responses include
descriptive messages and relevant codes. The Gateway uses request
identifiers and attaches trace IDs to facilitate debugging and
correlation across services.</p>
<h2 id="connectors-integrations">Connectors &amp; Integrations</h2>
<p>ark‑os‑noa interacts with the outside world via
<strong>Adapters</strong> and <strong>Connectors</strong>. These modules
encapsulate authentication, rate limiting, and protocol details,
allowing the rest of the system to remain agnostic to third‑party
specifics.</p>
<h3 id="builtin-connectors">Built‑in Connectors</h3>
<ul>
<li><strong>GitHub Connector:</strong> Uses the GitHub API to search,
clone and pull repositories. It supports scoping by organisation or
repository and can read commit logs and PR metadata.</li>
<li><strong>CRM Connector:</strong> Provides read/write access to CRM
systems (e.g. Salesforce, HubSpot). Initially operates in shadow mode
(read‑only) via the CRM Strangler Proxy; write‑through can be toggled
per endpoint. Handles pagination, rate limits and authentication.</li>
<li><strong>Model Hub Connector:</strong> Interfaces with external model
repositories (e.g. Hugging Face). Supports pulling models, downloading
tokenizers and retrieving licences. Works in conjunction with the Model
Serving service.</li>
<li><strong>Other API Connectors:</strong> Additional connectors
(e.g. for Slack, Notion, Jira) can be added by implementing the Adapter
interface. Each connector is packaged as its own microservice or plugin
to preserve modularity.</li>
</ul>
<h3 id="internal-connectors">Internal Connectors</h3>
<ul>
<li><strong>Registry &amp; Object Store:</strong> Adapters communicate
with the private OCI registry and MinIO using signed URLs. They ensure
that images and artefacts are pushed/pulled securely and that content
addressing is respected.</li>
<li><strong>Database &amp; Vector Store:</strong> Adapters abstract
database interactions. They provide typed functions to query or insert
metadata, run logs and embeddings without exposing SQL directly to the
application logic.</li>
</ul>
<h2 id="frontend-admin-console">Front‑End (Admin Console)</h2>
<p>The <strong>Admin Console</strong> is a web interface built with
Next.js. Its primary function is to give administrators and power users
visibility and control over the system. Major features include:</p>
<ul>
<li><strong>Jobs Dashboard:</strong> Displays active and past digest
jobs, their statuses, progress bars and any errors. Users can drill down
into individual jobs to view their <code>profile.json</code>,
<code>system_card.md</code>, SBOMs and vulnerability reports.</li>
<li><strong>Capacities &amp; Capsules:</strong> Shows currently running
Capsules, their resource usage and health status. Offers controls to
spawn or destroy Capsules.</li>
<li><strong>Artefacts Explorer:</strong> Lists generated artefacts (zip
files, PDFs, embeddings, SBOMs). Allows downloading via signed URLs and
cross‑referencing to their origins.</li>
<li><strong>SBOM &amp; Security:</strong> Provides a dedicated section
to review SBOMs, vulnerabilities, licences and risk scores. Policies can
be configured here (e.g. accepted licence list, vulnerability severity
thresholds).</li>
<li><strong>Model Registry &amp; Selector:</strong> Displays available
models, their metadata, benchmarks and usage statistics. Administrators
can add models to the ingestion queue or deprecate existing ones. The
ModelSelector’s decisions and rationales are visible for
transparency.</li>
<li><strong>CRM Controls:</strong> Allows toggling of CRM endpoint modes
(shadow/write‑through), viewing recent calls, and measuring divergence
between external CRM data and internal state.</li>
<li><strong>Settings &amp; Feature Flags:</strong> Provides toggles for
enabling/disabling optional services (NATS, Supabase, vcluster) and
adjusting environment variables. Also offers secret rotation and
certificate management.</li>
</ul>
<h2 id="interaction-patterns">Interaction Patterns</h2>
<ul>
<li><strong>External Clients:</strong> Use the Gateway API to submit
work. They receive job IDs and can query progress or results.
Authentication tokens limit access based on roles.</li>
<li><strong>Internal Agents:</strong> Call endpoints via Adapters. For
example, a CommandChiefAgent may call <code>/digest</code> to start
digestion for a new source or <code>/models/ingest</code> to add an
in‑house model. Internal calls attach run IDs and context for
traceability.</li>
<li><strong>Front‑End Users:</strong> Access the Admin Console to
monitor and control the system. When they trigger actions (e.g. toggling
a CRM endpoint), the console issues calls to the Gateway API on their
behalf.</li>
</ul>
<p>By exposing a clear API and a rich front‑end, ark‑os‑noa ensures that
humans and agents can seamlessly interact with the system, inspect its
state and adapt its behaviour without compromising security or
traceability. # Intelligence &amp; Learning in ark‑os‑noa</p>
<h2 id="vision">Vision</h2>
<p>ark‑os‑noa aspires to be more than an automation platform—it aims to
embody <strong>agentic intelligence</strong>. Intelligence here means
the ability to understand complex systems (codebases, data sets, SaaS
integrations), reason about them, learn from past executions, anticipate
future scenarios, and adapt models and workflows accordingly. Learning
is achieved through a combination of semantic understanding (knowledge
graphs and embeddings), model evaluation, feedback loops and simulation
of alternative futures (“branchwise foresight”).</p>
<h2 id="semantic-understanding">Semantic Understanding</h2>
<p>At the heart of ark‑os‑noa’s intelligence lies a <strong>semantic
representation</strong> of the world:</p>
<ul>
<li><strong>Knowledge Graphs:</strong> Built by the Graph Extract and
Digest services, these graphs link code symbols, data entities, API
endpoints, configuration keys and other artefacts. They capture
relationships (calls, imports, reads/writes, dependency edges) and
annotate nodes with metadata (e.g. licence, language, risk). Knowledge
graphs enable graph‑based queries and reasoning—answering questions like
“Which services write to table X?” or “What code paths handle payment
processing?”</li>
<li><strong>Embeddings &amp; Vector DB:</strong> The Embeddings Service
converts source code, documentation and natural‑language descriptions
into high‑dimensional vectors. Stored in pgvector or Qdrant, these
vectors power similarity search and clustering, enabling retrieval of
semantically related items even if keywords differ.</li>
</ul>
<h2 id="model-evaluation-evolution">Model Evaluation &amp;
Evolution</h2>
<p>The <strong>ModelSelectorAgent</strong> plays a central role in
learning. By recording the performance (latency, cost, accuracy) and
outcomes of each model used for a task, the system builds a knowledge
base of model behaviours. Over time, the selector’s heuristics can be
tuned or even replaced by learned policies that maximise utility subject
to constraints. Benchmark results and feedback loops allow the system to
retire underperforming models and onboard new ones seamlessly.</p>
<h2 id="feedback-loops-trace-learning">Feedback Loops &amp; Trace
Learning</h2>
<p>Every execution produces a <strong>Trace</strong>—a record of inputs,
actions, decisions, outputs and outcomes. These traces are stored in
Postgres along with logs and metrics. Post‑run analyses mine these
traces to identify patterns:</p>
<ul>
<li><strong>Success patterns:</strong> Which workflows succeeded quickly
with minimal retries? Which models performed best on certain task
types?</li>
<li><strong>Failure modes:</strong> Which tasks frequently hit policy
violations or vulnerabilities? Which connectors are unreliable?</li>
<li><strong>Cost hot‑spots:</strong> Where is budget being spent? Are
there cheaper alternatives?</li>
</ul>
<p>Insights from these analyses can feed back into NOA’s planning and
ModelSelector policies, closing the loop between execution and
learning.</p>
<h2 id="mind-maps-branchwise-foresight">Mind Maps &amp; Branchwise
Foresight</h2>
<p>The system leverages the knowledge graph and embeddings to construct
<strong>mind maps</strong>—visual or conceptual maps of relationships
between components, tasks and dependencies. These maps assist in
reasoning about the impact of changes, identifying missing connections
and planning new integrations.</p>
<p><strong>Branchwise foresight</strong> refers to simulating multiple
potential futures or scenarios before committing resources. For example,
before migrating a CRM function internally, NOA can instruct a
MicroAgentStack to:</p>
<ol type="1">
<li><strong>Simulate Strategy A:</strong> Keep the external CRM; use the
strangler proxy in shadow mode; measure divergence.</li>
<li><strong>Simulate Strategy B:</strong> Implement a minimal internal
replacement for a specific endpoint; run synthetic load; compare latency
and correctness.</li>
<li><strong>Simulate Strategy C:</strong> Replace the CRM entirely with
internal modules and measure performance, cost and user impact.</li>
</ol>
<p>By comparing the outcomes of these branches, NOA and the Board Agents
can choose a course of action informed by data rather than intuition.
This approach aligns with the idea of <strong>compound AI
systems</strong>, where tasks are decomposed into specialised modules
and their outputs orchestrated【438618440126565†L248-L292】.</p>
<h2 id="continuous-learning-improvement">Continuous Learning &amp;
Improvement</h2>
<p>Learning in ark‑os‑noa is continuous:</p>
<ul>
<li><strong>Auto‑patch loops:</strong> When tests fail, Graph Extract
proposes diffs, Runner applies them, and Safety verifies the fixes.
Successful patches can be proposed back to source repositories as pull
requests.</li>
<li><strong>Change intelligence:</strong> Scheduled self‑digests detect
changes in upstream sources; the system predicts breaking changes and
generates migration guides.</li>
<li><strong>Policy refinement:</strong> The Board and NOA adjust
policies (licence lists, vulnerability thresholds, model selection
heuristics) based on operational data and emerging requirements.</li>
</ul>
<p>By combining semantic representations, model analytics, feedback
loops and foresight simulations, ark‑os‑noa evolves beyond a static
workflow runner into an adaptive system capable of strategic reasoning
and self‑improvement.</p>
</body>
</html>
